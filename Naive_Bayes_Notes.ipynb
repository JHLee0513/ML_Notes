{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code is taken from an already existing example code with my own notes for educational purposes.\n",
    "\n",
    "Naive Bayes, due to its interesting trait where it is told to assume all features are independent, thus equally weighted, \n",
    "is known as a very quick algorithm.\n",
    "\n",
    "Obvisouly, naive bayes uses the Bayes theorem of conditional probability. \n",
    "\n",
    "Pros:\n",
    "    - It is easy and fast to predict class of test data set. It also perform well in multi class prediction\n",
    "    - Naive Bayes classifer often works better than other classifers such as logistic regression and requires \n",
    "      less training data\n",
    "    - Performs very well in the case of categorical outputs, thus classification of output classes. Normal, or Gaussian,\n",
    "      distribution is used in the case of numerical data for probability calculation.\n",
    "Cons:\n",
    "    - zero frequency error: often holds problem where it cannot calculate when there is a zero frequency in a category.\n",
    "      This is solved by increasing the frequency of every category by 1. (Easiest but not the best fix)\n",
    "    - It is also known as a bad estimator, compared to its speed.\n",
    "    - Since it uses independent predictors, meaning every feature hold equal/independent significance. This may lead to\n",
    "      a very bad prediction as that assumption cannot be applied to real life.\n",
    "      \n",
    "Three types of NB models are available under the scikit library:\n",
    "    -Gaussian: It is used in classification and it assumes that features follow a normal distribution.\n",
    "    -Multinomial: It is used for discrete counts. For example, let’s say,  we have a text classification problem.\n",
    "     Here we can consider bernoulli trials which is one step further and instead of “word occurring in the document”,\n",
    "     we have “count how often word occurs in the document”, you can think of it as “number of times outcome\n",
    "     number x_i is observed over the n trials”.\n",
    "    -Bernoulli: The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). \n",
    "     One application would be text classification with ‘bag of words’ model where the 1s & 0s are\n",
    "     “word occurs in the document” and “word does not occur in the document” respectively.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of Gaussian Model\n",
    "\n",
    "#Import Library of Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "#assigning predictor and target variables\n",
    "x= np.array([[-3,7],[1,5], [1,2], [-2,0], [2,3], [-4,0], [-1,1], [1,1], [-2,2], [2,7], [-4,1], [-2,7]])\n",
    "y = np.array([3, 3, 3, 3, 4, 3, 3, 4, 3, 4, 4, 4])\n",
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets \n",
    "model.fit(x, y)\n",
    "\n",
    "#Predict Output \n",
    "predicted= model.predict([[1,2],[3,4]])\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tips on producing a better NB Classifier:\n",
    "    - If continuous features do not have normal distribution,\n",
    "      we should use transformation or different methods to \n",
    "      convert it in normal distribution.\n",
    "    - If test data set has zero frequency issue, \n",
    "      apply smoothing techniques “Laplace Correction” \n",
    "      to predict the class of test data set.\n",
    "    - Remove correlated features, as the highly correlated features\n",
    "      are voted twice in the model and it can lead to over inflating importance.\n",
    "    - Naive Bayes classifiers has limited options for parameter tuning like alpha=1\n",
    "      for smoothing, fit_prior=[True|False] to learn class prior probabilities or not\n",
    "      and some other options. I would recommend to focus on your \n",
    "      pre-processing of data and the feature selection.\n",
    "    - You might think to apply some classifier combination technique like ensembling,\n",
    "      bagging and boosting but these methods would not help. Actually, “ensembling, \n",
    "      boosting, bagging” won’t help since their purpose is to reduce variance. \n",
    "      Naive Bayes has no variance to minimize.\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
