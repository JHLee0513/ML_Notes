{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ensemble modeling has now become crucial to building accurate and high-performing predictive models.\n",
    "Here are some tips/knowledge on it.\n",
    "\n",
    "1. What is an ensemble model?\n",
    "\n",
    "Ensemble modeling is combination of multiple different individual models to produce a better, high functioning algorithm.\n",
    "It is also known as combination of weak learners and models to produce a strong learner. An example is random forest classifer, \n",
    "which uses a group of learning trees instead of a single CART model to produce a much stronger model.\n",
    "\n",
    "2. What are bagging, boosting, and stacking?\n",
    "\n",
    "Bagging (Bootstrap Aggregating)  is an ensemble method. First, we create random samples of the training data set (sub sets of training data set). Then, we build a classifier for each sample. Finally, results of these multiple classifiers are combined using average or majority voting. Bagging helps to reduce the variance error.\n",
    "\n",
    "Boosting provides sequential learning of the predictors. The first predictor is learned on the whole data set, while the following are learnt on the training set based on the performance of the previous one. It starts by classifying original data set and giving equal weights to each observation. If classes are predicted incorrectly using the first learner, then it gives higher weight to the missed classified observation. Being an iterative process, it continues to add classifier learner until a limit is reached in the number of models or accuracy. Boosting has shown better predictive accuracy than bagging, but it also tends to over-fit the training data as well. Most common example of boosting is AdaBoost and Gradient Boosting. You can also look at these articles to know more about boosting algorithms.\n",
    "\n",
    "Stacking works in two phases. First, we use multiple base classifiers to predict the class. Second, a new learner is used to combine their predictions with the aim of reducing the generalization error.\n",
    "\n",
    "3. Use multiple (and ideally different) models to generate a strong algorithm\n",
    "\n",
    "We can combine multiple models of same ML algorithms, but combining multiple predictions generated by different algorithms would normally give you better predictions. It is due to the diversification or independent nature as compared to each other. For example, the predictions of a random forest, a KNN, and a Naive Bayes may be combined to create a stronger final prediction set as compared to combining three random forest model. The key to creating a powerful ensemble is model diversity. An ensemble with two techniques that are very similar in nature will perform poorly than a more diverse model set.\n",
    "\n",
    "4. How can we identify the weights of different models for ensemble?\n",
    "\n",
    "One of the most common challenge with ensemble modeling is to find optimal weights to ensemble base models. In general, we assume equal weight for all models and takes the average of predictions. But, is this the best way to deal with this challenge?\n",
    "There are various methods to find the optimal weight for combining all base learners. These methods provide a fair understanding about finding the right weight. I am listing some of the methods below:\n",
    "Find the collinearity between base learners and based on this table, then identify the base models to ensemble. After that look at the cross validation score (ratio of score) of identified base models to find the weight.\n",
    "Find the algorithm to return the optimal weight for base learners. You can refer article Finding Optimal Weights of Ensemble Learner using Neural Network to look at the method to find optimal weight.\n",
    "We can also solve the same problem using methods like: \n",
    "Forward Selection of learners\n",
    "Selection with Replacement\n",
    "Bagging of ensemble methods\n",
    "You can also look at the winning solution of Kaggle / data science competitions to understand other methods to deal with this challenge.\n",
    "\n",
    "\n",
    "Ensemble modeling in general provides a much more accurate and stable model compared to using a single model/algorithm.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
