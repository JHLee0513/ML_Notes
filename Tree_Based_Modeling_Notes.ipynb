{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is my note for tree based modeling of machine learning.\n",
    "\n",
    "Types of decision tree is based on the type of target variable we have. It can be of two types:\n",
    "\n",
    "Categorical Variable Decision Tree: Decision Tree which has categorical target variable then it\n",
    "                                    called as categorical variable decision tree. Example:- In above scenario of student problem,\n",
    "                                    where the target variable was “Student will play cricket or not” i.e. YES or NO.\n",
    "                                    \n",
    "Continuous Variable Decision Tree: Decision Tree has continuous target variable then it is called as Continuous \n",
    "                                   Variable Decision Tree.\n",
    "Advantages\n",
    "-Easy to Understand: Decision tree output is very easy to understand even for people from non-analytical background.\n",
    "                     It does not require any statistical knowledge to read and interpret them. Its graphical representation\n",
    "                     is very intuitive and users can easily relate their hypothesis.\n",
    "-Useful in Data exploration: Decision tree is one of the fastest way to identify most significant variables and relation\n",
    "                             between two or more variables. With the help of decision trees, we can create new \n",
    "                             variables / features that has better power to predict target variable. You can refer article\n",
    "                             (Trick to enhance power of regression model) for one such trick.  It can also be used in data\n",
    "                             exploration stage. For example, we are working on a problem where we have information available\n",
    "                             in hundreds of variables, there decision tree will help to identify most significant variable.\n",
    "-Less data cleaning required: It requires less data cleaning compared to some other modeling techniques. It is not influenced\n",
    "                              by outliers and missing values to a fair degree.\n",
    "-Data type is not a constraint: It can handle both numerical and categorical variables.\n",
    "-Non Parametric Method: Decision tree is considered to be a non-parametric method. This means that decision trees\n",
    "                        have no assumptions about the space distribution and the classifier structure.\n",
    " \n",
    "Disadvantages\n",
    "-Over fitting: Over fitting is one of the most practical difficulty for decision tree models. This problem gets solved\n",
    "               by setting constraints on model parameters and pruning (discussed in detailed below).\n",
    "-Not fit for continuous variables: While working with continuous numerical variables, decision tree looses information\n",
    "               when it categorizes variables in different categories.\n",
    "               \n",
    "               \n",
    "               \n",
    "In Random Forest, we grow multiple trees as opposed to a single tree. After training and predicting it takes the average\n",
    "of output from all trees.\n",
    "\n",
    "Assume number of cases in the training set is N. Then, sample of these N cases is taken at random but with replacement.\n",
    "This sample will be the training set for growing the tree.\n",
    "If there are M input variables, a number m<M is specified such that at each node, m variables are selected at random out\n",
    "of the M. The best split on these m is used to split the node. The value of m is held constant while we grow the forest.\n",
    "Each tree is grown to the largest extent possible and  there is no pruning.\n",
    "Predict new data by aggregating the predictions of the ntree trees (i.e., majority votes for classification, average for\n",
    "regression)\n",
    "\n",
    "ADV:\n",
    "This algorithm can solve both type of problems i.e. classification and regression and does a decent estimation at both fronts.\n",
    "One of benefits of Random forest which excites me most is, the power of handle large data set with higher dimensionality. It can handle thousands of input variables and identify most significant variables so it is considered as one of the dimensionality reduction methods. Further, the model outputs Importance of variable, which can be a very handy feature (on some random data set).\n",
    "\n",
    "It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n",
    "It has methods for balancing errors in data sets where classes are imbalanced.\n",
    "The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views \n",
    "and outlier detection.\n",
    "Random Forest involves sampling of the input data with replacement called as bootstrap sampling. Here one third of the data\n",
    "is not used for training and can be used to testing. These are called the out of bag samples. Error estimated on these out\n",
    "of bag samples is known as out of bag error. Study of error estimates by Out of bag, gives evidence to show that the out-of-bag\n",
    "estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate\n",
    "removes the need for a set aside test set.\n",
    "\n",
    "Dis ADV:\n",
    "It surely does a good job at classification but not as good as for regression problem as it does not give precise\n",
    "continuous nature predictions. In case of regression, it doesn’t predict beyond the range in the training data, and\n",
    "that they may over-fit data sets that are particularly noisy.\n",
    "Random Forest can feel like a black box approach for statistical modelers – you have very little control on what the\n",
    "model does. You can at best – try different parameters and random seeds!\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from sklearn.ensemble import RandomForestClassifier #use RandomForestRegressor for regression problem\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create Random Forest object\n",
    "model= RandomForestClassifier(n_estimators=1000)\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X, y)\n",
    "#Predict Output\n",
    "predicted= model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In the case of weak learners, meaning rules to use for training and prediction that are not good indicators, the algorithm\n",
    "may use a boosting algorithm in develop stronger rules from the base weak ones.\n",
    "\n",
    "Advantages of XGboost or GBM::\n",
    "\n",
    "Regularization: \n",
    "Standard GBM implementation has no regularization like XGBoost, therefore it also helps to reduce overfitting.\n",
    "In fact, XGBoost is also known as ‘regularized boosting‘ technique.\n",
    "Parallel Processing: \n",
    "XGBoost implements parallel processing and is blazingly faster as compared to GBM.\n",
    "But hang on, we know that boosting is sequential process so how can it be parallelized? We know that each tree can be built only after the previous one, so what stops us from making a tree using all cores? I hope you get where I’m coming from. Check this link out to explore further.\n",
    "XGBoost also supports implementation on Hadoop.\n",
    "High Flexibility \n",
    "XGBoost allow users to define custom optimization objectives and evaluation criteria.\n",
    "This adds a whole new dimension to the model and there is no limit to what we can do.\n",
    "Handling Missing Values \n",
    "XGBoost has an in-built routine to handle missing values.\n",
    "User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.\n",
    "Tree Pruning: \n",
    "A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm.\n",
    "XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.\n",
    "Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.\n",
    "Built-in Cross-Validation \n",
    "XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n",
    "This is unlike GBM where we have to run a grid-search and only a limited values can be tested.\n",
    "Continue on Existing Model \n",
    "User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.\n",
    "GBM implementation of sklearn also has this feature so they are even on this point.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBM sudo code for 2 classes:\n",
    "'''\n",
    "1. Initialize the outcome\n",
    "2. Iterate from 1 to total number of trees\n",
    "  2.1 Update the weights for targets based on previous run (higher for the ones mis-classified)\n",
    "  2.2 Fit the model on selected subsample of data\n",
    "  2.3 Make predictions on the full set of observations\n",
    "  2.4 Update the output with current results taking into account the learning rate\n",
    "3. Return the final output.\n",
    "\n",
    "The Parameters of GBM:\n",
    "learning_rate \n",
    "This determines the impact of each tree on the final outcome (step 2.4). GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates.\n",
    "Lower values are generally preferred as they make the model robust to the specific characteristics of tree and thus allowing it to generalize well.\n",
    "Lower values would require higher number of trees to model all the relations and will be computationally expensive.\n",
    "n_estimators \n",
    "The number of sequential trees to be modeled (step 2)\n",
    "Though GBM is fairly robust at higher number of trees but it can still overfit at a point. Hence, this should be tuned using CV for a particular learning rate.\n",
    "subsample \n",
    "The fraction of observations to be selected for each tree. Selection is done by random sampling.\n",
    "Values slightly less than 1 make the model robust by reducing the variance.\n",
    "Typical values ~0.8 generally work fine but can be fine-tuned further.\n",
    "Apart from these, there are certain miscellaneous parameters which affect overall functionality:\n",
    "loss \n",
    "It refers to the loss function to be minimized in each split.\n",
    "It can have various values for classification and regression case. Generally the default values work fine. Other values should be chosen only if you understand their impact on the model.\n",
    "init \n",
    "This affects initialization of the output.\n",
    "This can be used if we have made another model whose outcome is to be used as the initial estimates for GBM.\n",
    "random_state \n",
    "The random number seed so that same random numbers are generated every time.\n",
    "This is important for parameter tuning. If we don’t fix the random number, then we’ll have different outcomes for subsequent runs on the same parameters and it becomes difficult to compare models.\n",
    "It can potentially result in overfitting to a particular random sample selected. We can try running models for different random samples, which is computationally expensive and generally not used.\n",
    "verbose \n",
    "The type of output to be printed when the model fits. The different values can be: \n",
    "0: no output generated (default)\n",
    "1: output generated for trees in certain intervals\n",
    ">1: output generated for all trees\n",
    "warm_start \n",
    "This parameter has an interesting application and can help a lot if used judicially.\n",
    "Using this, we can fit additional trees on previous fits of a model. It can save a lot of time and you should explore this option for advanced applications\n",
    "presort  \n",
    " Select whether to presort data for faster splits.\n",
    "It makes the selection automatically by default but it can be changed if needed.\n",
    "\n",
    "'''\n",
    "\n",
    "#import libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier #For Classification\n",
    "from sklearn.ensemble import GradientBoostingRegressor #For Regression\n",
    "#use GBM function\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGboost is lot more comprehensive and intense but stronger. Find tutorials online when needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
